import torch
from typing import Callable, List

class BeamSearchPlanner:
    """
    The Planner for the SFP framework, using the Beam Search algorithm.

    Workflow:
    1. Receives the action `action` generated by the Agent and the current state `s_t`.
    2. Uses a pre-trained and frozen Generative World Model (GWM) to "imagine" K diverse 
       future states based on the action (generated by the GWM's Top-K VQ mechanism).
    3. These K future states serve as the initial candidate set (beam) for the beam search.
    4. Each future state in the candidate set is evaluated using a non-differentiable reward function.
    5. Finally, it selects and returns the future with the highest reward from all evaluated futures, 
       which serves as a pseudo-label.

    Note: In this single-step prediction setting, the beam search is simplified to a one-time 
    evaluation of the K direct candidate futures generated by the GWM. If extended to multi-step 
    prediction, an iterative search would be required.
    """
    def __init__(self, gwm: torch.nn.Module, reward_fn: Callable, beam_width: int):
        """
        Initializes the Planner.

        Args:
            gwm (torch.nn.Module): The pre-trained and frozen Generative World Model.
            reward_fn (Callable): The reward function. It should accept two arguments 
                                  (prediction, target) and return a scalar reward value.
            beam_width (int): The width of the beam search (B). This value should be less 
                              than or equal to the top_k value configured in the GWM.
        """
        self.gwm = gwm
        self.reward_fn = reward_fn
        self.beam_width = beam_width
        
        # Ensure the GWM is in evaluation mode and its gradients are not computed. 
        # This is crucial for efficiency and correctness.
        self.gwm.eval()
        for param in self.gwm.parameters():
            param.requires_grad = False

    @torch.no_grad()
    def plan(self, s_t: torch.Tensor, action: torch.Tensor, ground_truth: torch.Tensor) -> torch.Tensor:
        """
        Executes a single-step plan to find the highest-reward future state (pseudo-label) 
        for a given state and action.

        Args:
            s_t (torch.Tensor): The current state, with shape [B, T_in, C, H, W].
            action (torch.Tensor): The action generated by the agent, with shape [B, embedding_dim, H', W'].
            ground_truth (torch.Tensor): The true future state, with shape [B, T_out, C, H, W].
                                         It is only used for calculating the reward and is not revealed to the agent.

        Returns:
            torch.Tensor: The highest-reward future state (pseudo-label), with the same shape as ground_truth.
        """
        # --- 1. Use GWM to "imagine" futures ---
        # The gwm.imagine_futures() method generates a list of K candidate futures based on 
        # s_t (as a condition) and action (as a latent space intention).
        # Note: The batch size (B) here should ideally be 1, as planning is done per sample.
        # If the batch size > 1, it requires iterating through the batch or adjusting the GWM's output.
        if s_t.size(0) > 1:
            # If it's a batch, we'll plan for each sample individually
            best_futures_batch = []
            for i in range(s_t.size(0)):
                s_t_i = s_t[i:i+1]
                action_i = action[i:i+1]
                gt_i = ground_truth[i:i+1]
                best_future_i = self._plan_single_sample(s_t_i, action_i, gt_i)
                best_futures_batch.append(best_future_i)
            return torch.cat(best_futures_batch, dim=0)
        else:
            return self._plan_single_sample(s_t, action, ground_truth)

    def _plan_single_sample(self, s_t_single: torch.Tensor, action_single: torch.Tensor, gt_single: torch.Tensor) -> torch.Tensor:
        """
        Performs planning for a single sample.
        """
        imagined_futures: List[torch.Tensor] = self.gwm.imagine_futures(s_t_single, action_single)

        # Trim candidates to beam_width to not exceed GWM's top_k
        candidates = imagined_futures[:self.beam_width]

        best_future = None
        best_reward = -float('inf')

        # --- 2. Evaluate each candidate future ---
        for future_candidate in candidates:
            # Calculate the reward for the current candidate future
            # reward_fn(prediction, target)
            reward = self.reward_fn(future_candidate, gt_single)
            
            # --- 3. Maintain the best choice ---
            if reward > best_reward:
                best_reward = reward
                best_future = future_candidate
        
        # If no best future was found,
        # return the first candidate to ensure there is always an output.
        if best_future is None:
            best_future = candidates[0]
            
        return best_future